{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bank churn prediction using ANN** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aims:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing project dependencies \n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data warehousing \n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Exploratory Data Analysis & Visualisation \n",
    "%matplotlib inline\n",
    "import pandas_profiling\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature Engineering and Modelling \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Model improvement and Evaluation \n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "# Model export for deployment \n",
    "import pickle\n",
    "\n",
    "# Supressing warning messages \n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **----------------------------------  1. Data collection  -----------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data through API \n",
    "!kaggle datasets download -d mrmorj/big-mart-sales -p ..\\Data --unzip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "data = pd.read_csv('BankCustomers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing first 5 rows of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the data types of the columns\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing dataset shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st check for null values and datatype check \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd check for number of null values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique count for each variable\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing summary of statistics for numeric columns\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (SQL standard) Formatting column headers by removing potential capital letters and spaces in column headers \n",
    "data.columns = data.columns.str.lower()\n",
    "data.columns = data.columns.str.replace(' ','_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **----------------------------------  3. Data Warehousing  -----------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to warehouse data in a Postgre database \n",
    "def store_data(data,tablename):\n",
    "    \"\"\"\n",
    "    :param data: variable, enter name of dataset you'd like to warehouse\n",
    "    :param tablename: str, enter name of table for data \n",
    "    \"\"\"\n",
    "\n",
    "    # Saving cleaned data as csv\n",
    "    data.to_csv(f'../Data/{tablename}_clean.csv', index=False)\n",
    "\n",
    "    # Engine to access postgre\n",
    "    engine = create_engine('postgresql+psycopg2://postgres:password@localhost:5432/projectsdb')\n",
    "\n",
    "    # Loads dataframe into PostgreSQL and replaces table if it exists\n",
    "    data.to_sql(f'{tablename}', engine, if_exists='replace',index=False)\n",
    "\n",
    "    # Confirmation of ETL \n",
    "    return(\"ETL successful, {num} rows loaded into table: {tb}.\".format(num=len(data.iloc[:,0]), tb=tablename))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling store_data function to warehouse cleaned data\n",
    "store_data(data,\"p6_churnprediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **----------------------------------  4. Exploratory data analysis  -----------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking distribution of categorical fields \n",
    "print(data.geography.value_counts())\n",
    "print(data.gender.value_counts())\n",
    "print(data.exited.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing summary of statistics for numeric columns\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting my custom color palette\n",
    "colors = [\"#32CD32\",\"#FF0000\"]\n",
    "sns.set_palette(sns.color_palette(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totretained = data[data['exited'] == 0]['exited'].count() / data.shape[0] * 100\n",
    "totlost = data[data['exited'] == 1]['exited'].count() / data.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing distribtuon of target values \n",
    "fig, ax = plt.subplots()\n",
    "plt.figure(figsize=(8,6),facecolor='white')\n",
    "sns.countplot(x='exited', data=data)\n",
    "plt.xticks([0, 1], ['Retained', 'Lost'])\n",
    "plt.xlabel('Condition', size=15, labelpad=12, color='grey')\n",
    "plt.ylabel('Amount of customers', size=15, labelpad=12, color='grey')\n",
    "plt.title(\"Proportion of customers lost and retained\", size=15, pad=20)\n",
    "plt.ylim(0, 9000)\n",
    "plt.text(-0.12, 7000, f\"{round(totretained, 2)}%\", fontsize=12,weight='bold')\n",
    "plt.text(0.90, 1000, f\"{round(totlost, 2)}%\", fontsize=12,weight='bold')\n",
    "sns.despine()\n",
    "plt.savefig('../images/Churn_barchart_distrib.png')\n",
    "plt.close(1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting all the above results in a single visualization\n",
    "\n",
    "cat_data=['geography', 'gender', 'tenure','numofproducts', 'hascrcard','isactivemember']\n",
    "q=1\n",
    "plt.figure(figsize=(16,12),facecolor='white')\n",
    "# Plot a grid with count plots of all categorical variables\n",
    "for i in cat_data:\n",
    "    plt.subplot(2,3,q)\n",
    "    ax=sns.countplot(data[i],hue=data.exited)\n",
    "    plt.xlabel(i)\n",
    "    q+=1\n",
    "\n",
    "plt.savefig('../images/independentfeatures_distrib.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relations based on the continuous data attributes\n",
    "fig, axarr = plt.subplots(3, 2,facecolor='white',figsize=(16,12))\n",
    "sns.boxplot(y='creditscore',x = 'exited', hue = 'exited',data = data, ax=axarr[0][0])\n",
    "sns.boxplot(y='age',x = 'exited', hue = 'exited',data = data , ax=axarr[0][1])\n",
    "sns.boxplot(y='tenure',x = 'exited', hue = 'exited',data = data, ax=axarr[1][0])\n",
    "sns.boxplot(y='balance',x = 'exited', hue = 'exited',data = data, ax=axarr[1][1])\n",
    "sns.boxplot(y='numofproducts',x = 'exited', hue = 'exited',data = data, ax=axarr[2][0])\n",
    "sns.boxplot(y='estimatedsalary',x = 'exited', hue = 'exited',data = data, ax=axarr[2][1])\n",
    "plt.savefig('../images/boxplots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation of each variable\n",
    "# data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing correlations of each features in dataset \n",
    "datacorr = round(data.corr(),2)\n",
    "corrmat = datacorr\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(16,12),facecolor='white')\n",
    "\n",
    "# Plotting heat map\n",
    "plot = sns.heatmap(datacorr,annot=True,cmap=\"RdYlGn\")\n",
    "plt.savefig('../images/churn_correlation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile report of each variable\n",
    "# pandas_profiling.ProfileReport(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **----------------------------------  5. Feature engineering  -----------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping fields that wont benefit the model\n",
    "data.drop(labels=['rownumber','customerid','surname'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encoding for remaining categorical fields \n",
    "data = pd.get_dummies(data, drop_first = False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dependent and independent features to apply scaling\n",
    "X = data.drop([\"exited\"],axis=1)\n",
    "\n",
    "# Dependent feature | Target variable \n",
    "y= data['exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train test split to split train and test data | Stratifying so y_test can reflect y_train.Resulting in a more realistic simulation of how the model is going to perform on new data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.20, random_state=23, shuffle=True, stratify=y)\n",
    "\n",
    "# Viewing shape of train / test data\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **----------------------------------  6. Modelling  -----------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **----------------------------------  8. Evaluation -----------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy=accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'batch_size': [25, 32],\n",
    "              'epochs': [100, 500],\n",
    "              'optimizer': ['adam', 'rmsprop']}\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('BankCustomers.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "\n",
    "# convert categorical feature into dummy variables\n",
    "\n",
    "states=pd.get_dummies(X['Geography'],drop_first=True)\n",
    "gender=pd.get_dummies(X['Gender'],drop_first=True)\n",
    "\n",
    "#concatenate the remaining dummies columns\n",
    "X=pd.concat([X,states,gender],axis=1)\n",
    "\n",
    "#drop the columns as it is no longer required\n",
    "\n",
    "X=X.drop(['Geography','Gender'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Part 2 - Now let's make the ANN!\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)\n",
    "\n",
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy=accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'batch_size': [25, 32],\n",
    "              'epochs': [100, 500],\n",
    "              'optimizer': ['adam', 'rmsprop']}\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
